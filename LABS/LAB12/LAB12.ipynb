{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsq2fSU1dwdT"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y9qtPtXWdwdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761c8522-ff02-4969-b1f3-cdc91deac94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  4181k      0  0:00:19  0:00:19 --:--:-- 10.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6vtQzj85dwdU"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fnhXSSwPdwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe56b61-1c6c-4a7d-ff93-fd0ef9afafd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EfVhc-j_dwdU"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RFWun9fndwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dae99e3-afac-420e-e001-b656a9e43a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-26 16:31:15.486176: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-26 16:31:21.804493: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI2dsA_QdwdU"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DNQRs8oVdwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8759dcd2-0c0e-411d-8a3f-4030699bbbb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"This is the second film I've seen of Ida Lupino as a director after 53's the hitch-hiker. I think this one was a better film then that one. This one has a girl who is about to get married and she is then sexually assaulted and doesn't like everyone looking and talking about her so she runs away and and is taken in by a family. I think Leonard Maltin's review is right only to give it 2 and 1/2 stars.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot-u_oVxdwdU"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg_8j81BdwdU"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GB1qoeedwdU"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hTAwHAMndwdU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx5xSrpidwdU"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "epAwlMDKdwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130fce43-4e9b-43c1-8636-0f127866ed79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XB8y6todwdU"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ryq5YT4fdwdU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYVXJIKdwdU"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WOfFSAn1dwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b475c5d3-57f9-4a71-d06b-3d021e6ebfaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4350 - accuracy: 0.8158 - val_loss: 0.2857 - val_accuracy: 0.8926\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2961 - accuracy: 0.8899 - val_loss: 0.2778 - val_accuracy: 0.8974\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2669 - accuracy: 0.9046 - val_loss: 0.2858 - val_accuracy: 0.8956\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2544 - accuracy: 0.9123 - val_loss: 0.2926 - val_accuracy: 0.8964\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2419 - accuracy: 0.9165 - val_loss: 0.3032 - val_accuracy: 0.8956\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2355 - accuracy: 0.9199 - val_loss: 0.3106 - val_accuracy: 0.8948\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2349 - accuracy: 0.9257 - val_loss: 0.3274 - val_accuracy: 0.8938\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2274 - accuracy: 0.9265 - val_loss: 0.3319 - val_accuracy: 0.8924\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2362 - accuracy: 0.9255 - val_loss: 0.3369 - val_accuracy: 0.8896\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2280 - accuracy: 0.9255 - val_loss: 0.3429 - val_accuracy: 0.8894\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3014 - accuracy: 0.8832\n",
            "Test acc: 0.883\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsfS7QxVdwdU"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bCzh6eidwdU"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YeGWsu0ddwdU"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3iwA2mddwdU"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y4EwLFsadwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc0400d-334c-4a4c-d593-cb28e8d57e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3732 - accuracy: 0.8455 - val_loss: 0.2495 - val_accuracy: 0.9042\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2392 - accuracy: 0.9161 - val_loss: 0.2490 - val_accuracy: 0.9066\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2120 - accuracy: 0.9303 - val_loss: 0.2630 - val_accuracy: 0.9096\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1981 - accuracy: 0.9402 - val_loss: 0.2755 - val_accuracy: 0.9082\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9452 - val_loss: 0.2911 - val_accuracy: 0.9066\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9467 - val_loss: 0.2969 - val_accuracy: 0.9094\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.9500 - val_loss: 0.3057 - val_accuracy: 0.9094\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1726 - accuracy: 0.9507 - val_loss: 0.3162 - val_accuracy: 0.9078\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.9521 - val_loss: 0.3221 - val_accuracy: 0.9050\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1705 - accuracy: 0.9532 - val_loss: 0.3268 - val_accuracy: 0.9026\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.2693 - accuracy: 0.9013\n",
            "Test acc: 0.901\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SDPXETedwdU"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbCb5d1GdwdV"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g8YaZy_9dwdV"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km9maJCzdwdV"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zpOXj-GLdwdV"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXkh8JwhdwdV"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6b5o7mS3dwdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4502ff-6f91-48f6-b878-0d052a12ad0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5317 - accuracy: 0.7821 - val_loss: 0.3492 - val_accuracy: 0.8600\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8620 - val_loss: 0.3149 - val_accuracy: 0.8852\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2981 - accuracy: 0.8727 - val_loss: 0.3292 - val_accuracy: 0.8736\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2883 - accuracy: 0.8791 - val_loss: 0.3036 - val_accuracy: 0.8914\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2784 - accuracy: 0.8826 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2695 - accuracy: 0.8877 - val_loss: 0.3136 - val_accuracy: 0.8920\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2523 - accuracy: 0.8921 - val_loss: 0.3165 - val_accuracy: 0.8846\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2489 - accuracy: 0.8936 - val_loss: 0.3232 - val_accuracy: 0.8806\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2424 - accuracy: 0.8983 - val_loss: 0.3224 - val_accuracy: 0.8766\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2397 - accuracy: 0.9000 - val_loss: 0.3201 - val_accuracy: 0.8928\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8824\n",
            "Test acc: 0.882\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_oJSgdjGdwdV"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "UUYevKbadwdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1d5555-f571-4acf-ec8f-22468cf0c7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.28 percent positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"This is incredible. I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Model| Time taken to train|Accuracy|\n",
        "|:----:|:----:|:----:|\n",
        "|Binary Unigram Model|21 seconds|0.883|\n",
        "|Binary Bigram Model|12 seconds|0.901|\n",
        "|TF-IDF Bigram Model|19 seconds|0.882|"
      ],
      "metadata": {
        "id": "h899YjZAiygV"
      }
    }
  ]
}